import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import os

device = torch.device("mps")

# Transforms for images
pic_transform = transforms.Compose([transforms.Resize((224, 224)),
                                    transforms.ToTensor(),
                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])


# Step 1: Initialise ResNet-18 with pre-trained weights as a feature extractor
# Modify full connected(fc) layer with nn.Linear for 3-class classification
def get_shipWatcher():
    model = models.resnet18(weights="DEFAULT")
    for param in model.parameters():
        param.requires_grad = False
    input_size = model.fc.in_features
    model.fc = nn.Linear(input_size,3)
    return model


# Step 2: Create custom PyTorch dataset implementation utilising "lazy loading" for memory-efficient data pipelining

train_labels = []
train_images = []
val_labels = []
val_images = []
class_list = ["empty", "person", "vehicle"]

for label,image in enumerate(class_list):
    train_dir = os.path.join("./Data/train/",image)
    val_dir = os.path.join("./Data/val/",image)
    for file_name in os.listdir(train_dir):
        path = os.path.join(train_dir, file_name)
        train_images.append(path)
        train_labels.append(label)
    for file_name in os.listdir(val_dir):
        path = os.path.join(val_dir, file_name)
        val_images.append(path)
        val_labels.append(label)
class Data(Dataset):
    def __init__(self, dataset, transform=None):
        self.x = dataset[0]
        self.y = dataset[1]
        self.transform = transform
        self.len = len(dataset[0])

    def __getitem__(self, index):
        x = self.transform(Image.open(self.x[index]).convert("RGB"))
        y = self.y[index]
        return x, y

    def __len__(self):
        return self.len


# Step 3: Create objects with selected hyperparameters for training
shipWatcher = get_shipWatcher()
shipWatcher = shipWatcher.to(device)
criterion = nn.CrossEntropyLoss()
optimiser = optim.Adam(shipWatcher.fc.parameters(), lr=0.001, weight_decay=0.01)
train_dataset = Data([train_images, train_labels], transform=pic_transform)
val_dataset = Data([val_images, val_labels], transform=pic_transform)
train_loader = DataLoader(dataset=train_dataset, batch_size=5, shuffle=True)
val_loader = DataLoader(dataset=val_dataset, batch_size=len(val_images))


# Step 4: Define function for training, validating shipWatcher model
def train_shipWatcher(epochs):
    for epoch in range(epochs):
        shipWatcher.train()
        train_loss = 0
        val_accuracy = 0
        print("_________________")
        print(f"Epoch {epoch+1}")
        for x, y in train_loader:
            x = x.to(device)
            y = y.to(device)
            optimiser.zero_grad()
            yhat = shipWatcher(x)
            loss = criterion(yhat, y)
            loss.backward()
            optimiser.step()
            train_loss += loss.item()
        print(f"Training Loss: {train_loss}")
        shipWatcher.eval()
        with torch.no_grad():
            for x, y in val_loader:
                x = x.to(device)
                y = y.to(device)
                yhat = shipWatcher(x)
                val_accuracy = (torch.argmax(yhat, dim=1)==y).sum().item()
                print(f"{val_accuracy}/{len(val_images)}")

train_shipWatcher(20)


# Step 5: Save parameters of optimal model
torch.save(shipWatcher.state_dict(), "shipWatcher.pth")
